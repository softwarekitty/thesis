\subsection{Building the corpus of regexes}
\label{sec:corpusBuilding}

The goal of this experiment was to collect regexes from a variety of projects to represent the breadth of how developers use the language features.

\subsubsection{Selecting projects to mine for utilizations}
Using the GitHub API, \dbfetch{nProjScanned} projects containing Python code were mined for utilizations as described in Section~\ref{sec:miningImplementation}.  This section describes how these projects were selected.

Every time a new repository is created on GitHub, a new unique identifier (strictly greater than existing identifiers) is generated and assigned to that repository.  This work refers to these identifiers using the shorthand: \emph{repo ID}.  At the time the mining for utilizations used in this study was performed, the largest repo ID was between 32 million and 33 million.  Dividing these repo IDs into four groups each of size $2^{23} = 8,388,608$ (with the fourth group being a little larger than that), the second group, which spans the range 8,388,608 - 16,777,215 was split into 32 sections so that starting indices were 262,144 repo IDs apart.  The original intention was to mine the entire second $1/4$ of the first 32 million repo IDs, but due to the challenges described in Section~\ref{sec:miningChallenges}, only the first 100 or so projects from each of the 32 starting points was mined.  Instead of spending the majority of available time on perfecting a mining technique, the determination was made to analyze the data that had already been gathered.

\input{table/saturation}

\subsubsection{Saturation of artifacts with regexes}
Out of the \dbfetch{nProjScanned}\ projects scanned, \dbfetch{percentProjectsUsingRegex}\% (\dbfetch{nProjectsUsingRegex}) contained at least one utilization.  Within a project, a duplicate utilization was marked when two versions of the same file have the same function, pattern and flags.  In total, \dbfetch{nUsages} non-duplicate utilizations were observed.  To illustrate how saturated projects are with regexes, measurements are made for the number of utilizations per project, number of files scanned per project, number of files containing utilizations, and number of utilizations  per file, as shown in Table~\ref{table:saturation}.

Of projects containing at least one utilization, the average utilizations per project was 32 and the maximum  was 1,427.  The project with the most utilizations is a C\# project\footurl{https://github.com/Ouroboros/Arianrhod} that maintains a collection of source code for 20 Python libraries, including larger libraries like {\tt pip}, {\tt celery} and {\tt ipython}.  These larger Python libraries contain many utilizations.
From Table~\ref{table:saturation}, it can also be seen that each project had an average of 11 files containing any utilization, and each of these files had an average of 2 utilizations.

\begin{figure}
\centering
  \centering
  \includegraphics[width=.72\textwidth]{nontex/illustrations/partFlags.eps}
  \caption{Which behavioral flags are used?}
  \vspace{-6pt}
\label{fig:partFlags}
\end{figure}

\begin{figure}
\centering
  \centering
  \includegraphics[width=.70\textwidth]{nontex/illustrations/partFunctions.eps}
  \caption{Which behavioral flags are used?}
  \vspace{-6pt}
\label{fig:partFunctions}
\end{figure}

\subsubsection{Flags and functions}
As shown in figure ~\ref{fig:partFlags}, of all behavioral flags used, ignorecase (\DTLfetch{data}{key}{percentI}{value}\%) and multiline (\DTLfetch{data}{key}{percentM}{value}\%) were the most frequently used.  It is also worth noting that although multiple flags can be combined using a bitwise or, this was never observed.
When considering flag use, non-behavioral flags (default and debug) were excluded, which are present in \DTLfetch{data}{key}{percentFlags0}{value}\% of all \emph{utilizations}.

As seen in Figure~\ref{fig:partFunctions} The `compile' function encompasses \DTLfetch{data}{key}{percentCompile}{value}\% of all utilizations.  Regexes may be compiled in an attempt to improve performance (only compile once) or to abstract the regex from the rest of the code.  Compiled regexes are often observed at the top of a file, listed along with other highly-scoped variables maintained separately from blocks of code.  Using the other {\tt re} module functions in-line may be less preferred by developers because of the `magic strings' which could be refactored to a variable.

\subsubsection{Selecting a body of patterns from a set of utilizations}
To guarantee that the behavior of regexes used for analysis depended only on the pattern extracted from a utilization, the \dbfetch{percentBadFlags}\%  of utilizations using flags were excluded from further analysis.  An additional \dbfetch{percentInvalidPattern}\% of utilizations contained patterns that could not be compiled because the pattern was non-static (e.g., used some runtime variable).
All distinct patterns from the remaining \dbfetch{percentCleanUsages}\% (\dbfetch{nCleanUsages}) of utilizations were pre-processed by removing Python quotes (\verb!`\\W!' becomes \verb!\\W!), and unescaping escaped characters (\verb!\\W! becomes \verb!\W!).  After these filtering steps, \dbfetch{nDistinctPatterns} distinct patterns remained.

\subsubsection{Parsing Python Regular Expression patterns using a PCRE parser}
The collection of distinct patterns formed by this process was parsed into tokens using an ANTLR-based, open source PCRE parser\footurl{https://github.com/bkiers/pcre-parser}.  A comparison of the features supported by this parser (Perl features) and Python is provided in Table~\ref{table:rankedFeatureSupport}, and indicates that all but the ENDZ feature have identical syntax and meaning.  Fortunately, the syntax of the ENDZ feature (e.g., \cverb!R\Z!) matches the syntax of the LNLZ feature (e.g., \cverb!R\Z!) so that in practice, the parser used can correctly identify all studied features.  To clarify the difference, if a newline is the last character in a string, ENDZ will match after that newline, and LNLZ will match before that newline.

This parser was unable to support \dbfetch{percentUnicode}\% (\dbfetch{N_UNICODE}) of the patterns due to unsupported Unicode characters.  Another 0.1\% (17) of the patterns used PCRE features not valid in Python (see Section\todoNow{match with section} for more information on these features).  Two additional patterns used the commenting feature which is valid in Python but encountered so rarely that it is not included in the analysis of features.
%number used to be 25 from \dbfetch{N_ALIEN}, but 6 were more likely parsing confusion problems - added to `other error' count below to get 22 (was 16).  Percent also adjusted.

Details about the patterns excluded due to alien features used are provided here:
\begin{description} \itemsep -1pt
\item [IFC (If conditionals)] six patterns like \verb!"^(\()?([^()]+)(?(1)\))$"!
\item [NCND (Named conditions)] five patterns like \verb!"(?P<g2>b)?((?(g2)c|d))"!
\item [IFEC (If-else conditionals)] three patterns like \verb!"^(?:(a)|c)((?(1)b|d))$"!
\item [ECOM (Comments)] two patterns like \verb!"(?# Break or beginning)"!
\item [LHX (Long hex)] two patterns like \verb!"\uFF0E"!
\item [PXCC (Posix character classes)] one pattern containing the fragment \verb!"([[:alpha:]]+://)?"!
\end{description}

An additional 0.16\% (22) of the patterns were excluded because they were empty or otherwise malformed so as to cause a parsing error.

The \dbfetch{nCorpus} distinct pattern strings that remain were each assigned a weight value equal to the number of distinct projects the pattern appeared in.  We  refer to this set of weighted, distinct pattern strings as the \emph{corpus}.

\subsection{Analyzing the corpus of regexes}
\label{sec:corpusAnalyzing}

\subsubsection{Parsing feature tokens}
For each escaped pattern, the PCRE-parser produces a tree of feature tokens, which is converted to a vector by counting the number of each token in the tree.  For a simple example, consider the patterns in Figure~\ref{fig:featureParsing}.  The pattern \verb!"^m+(f(z)*)+"! contains four different types of tokens. It has the KLE operator (specified using the asterisk \verb!`*'!), the ADD operator (specified using the plus \verb!`+'!), two CG elements (specified using pairs of parenthesis \verb!`('! and \verb!`)'!), and the STR position (specified using the caret \verb!`^'!). A detailed description of all studied features is provided in Section~\ref{sec:featureDescriptions}.

\begin{figure}[tb]
\centering
\includegraphics[height=0.6in]{nontex/illustrations/featureParsing.eps}
\caption{Two patterns parsed into feature vectors}
\label{fig:featureParsing}
\vspace{-12pt}
\end{figure}

Once all patterns were transformed into vectors, each feature was examined independently for all patterns, tracking the number of patterns, files and projects that the each feature appears in at least once.

\input{table/features/featureStatsOnly}

\subsubsection{Feature usage within the corpus}
\label{sec:featureResults}
Table~\ref{table:featureStatsOnly} displays feature usage from the corpus in terms of the number of patterns, files and projects, as well as in terms of tokens used.

The first column, \emph{rank}, lists the rank of a feature (relative to other features) in terms of the number of projects in which it appears. The next column, \emph{code}, gives a succinct reference string for the feature as described in Section~\ref{sec:featureDescriptions}. The \emph{example} column provides a short example of how the feature can be used.The next six columns contain usage statistics providing a variety of perspectives on how frequently the features are used in the observed population.

The \emph{\% projects} column contains the percentage of projects  using a feature out of the 1,645 projects scanned that contain at least one pattern in the corpus.  The \emph{nProjects} column provides the number of projects that contain at least one usage of a feature.  Assuming that one project generally corresponds to some high-level goal of a programmer or a team of programmers, these values provide a sense of how frequently a feature is \emph{part of a software solution} in even the slightest way.  Because of the generality of this measure and the goal of this study to gauge how features of regular expressions are used in general, these values are used to determine the rank of a feature.

The \emph{nFiles} column specifies the number of files that contain at least one observed usage of the feature.  For reference, recall that a total of \dbfetch{nFilesUsingRegex} files were scanned that contain at least one feature usage.  Assuming that programmers organize code into separate files based on what the code needs to do, this number can provide insight into the variety of different conceptually separate \emph{task categories} a feature is used for.

The \emph{nPatterns} column contains the number of patterns in which a feature was observed. Each regex is compiled from a particular pattern and performs at least one function desired by a programmer.  Therefore the number of patterns composed using a feature can provide insight into the number of \emph{specific tasks} a feature is used for.

The \emph{nTokens} column, gives the total number of tokens observed for a feature, combining the token counts of all patterns in the corpus.  This value provides a sense of how often the language feature is used \emph{for any task}.

The last column, \emph{maxTokens}, gives the maximum number of times that a feature appears in a single regex.  Assuming that a feature that a programmer finds convenient is used more frequently in a given regex, this value provides a sense of \emph{convenience} provided by the feature.
