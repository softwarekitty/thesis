\subsection{Experimental design}

\subsubsection{Conceptual basis}
Regular expression languages are infinite and exhibit substantial variety, but programmers are likely to use them for a limited number of purposes.  The goal of this study is to identify categories of regular expression usage and the frequency of usage in these categories so that designers of regular expression languages and end-user tools can better support what is most useful to programmers.  This study employs a sequence of two categorization attempts to achieve its goal.
\begin{itemize}\itemsep -1pt
\item[1] determine an objective behavioral similarity score for all pairs of regexes, and find clusters of highly similar regexes, so that a large number of regexes can be seen as a modest number of clusters, and then manually categorize these clusters
\item[2] thoroughly examine the regexes to create a categorization technique informed by the behavioral categories determined in step 1, and then manually categorize regexes based on inspection alone
\end{itemize}

\subsubsection{Implementation details for }
\paragraph{Commonly observed categories} The following categories were observed after examining half of the corpus:
\begin{itemize}
\item[ L ] LONG     too long to deal with

Web stuff
\item[ b ] brackets  capturing or matching all content within brackets like \cverb!<.*?>! or \cverb!<[^>]*?>!
\item[ w ] web       parsing urls, IP addresses and web protocols
\item[ t ] tags      scanning for HTML tag content like `a href='

assignment
\item[ C ] (Capture) capture a variable assignment using `='
\item[ = ] operator  recognize a line of code (has `=', `+' or operators) without capturing

non-free long strings
\item[ e ] error     scanning for an expected error message prefix like `Invalid object:'
\item[ o ] or        scanning for keywords using an OR like password|secret|hash
\item[ k ] keyword   recognize code keywords without operators or an OR of keywords
\item[ l ] label     finding a plain label like `LIST OF EDGES' or `WIN_UTILS'

semi-free strings with delimiters
\item[ x ] extension  recognize a filename or extension
\item[ f ] file       recognize a file path with particular contents
\item[ r ] row        recognize some line that is a log file or row

mostly-free strings
\item[ i ] identifier    identifiers (alphanumeric) with a restriction (must be capitol first, have a `@')
\item[ p ] punctuation   finding a punctuation for de-serialization, like `:',`_',`|'
\item[ d ] dates         parsing a simple string code like a date format
\item[ n ] numbers       parsing a phone number, ssn, numeric date, regular number
\item[ a ] anchored         free, except anchored on one or both sides like \cverb!^\\d+\$!


completely free strings
\item[ u ] unicode     hex ranges, probably looking for Unicode
\item[ s ] space       whitespace only - no intent detectable
\item[ v ] vanilla     digits and words or slashes, optionally space - no intent detectable
\item[ y ] why?        does not do anything - no intent detectable


\todoMid{continue this intro}

\paragraph{Determining behavioral similarity} An ideal analysis of regex behavioral similarity would use subsumption or containment analysis. However, a tool that could facilitate an analysis of the corpus could not be found.  This is likely due to features like back-references, positional anchors and look-arounds that are ubiquitously excluded from the feature sets of regex analysis tools that are able to build automata and thereby perform containment analysis.  For this reason, a new technique using string matching was developed that can create a similarity score between two regexes with existing technology.

\begin{figure}[tb]
\centering
\includegraphics[height=0.6in]{nontex/illustrations/minimalMatrix.eps}
\caption{A similarity matrix created by counting strings matched}
\label{fig:minimalMatrix}
\end{figure}

\begin{figure}[tb]
\centering
\includegraphics[width=0.7\columnwidth]{nontex/illustrations/matrixToGraph.eps}
\vspace{-6pt}
\caption{Creating a similarity graph from a similarity matrix}
\vspace{-6pt}
\label{fig:matrixToGraph}
\end{figure}



\subsubsection{Overview of process}

The similarity analysis used in this study clusters regular expressions by their behavioral similarity on matched strings.
Consider two unspecified patterns {\tt A} and {\tt B}, a set {\tt mA} of 100 strings that pattern {\tt A} matches, and a set {\tt mB} of 100 strings that pattern {\tt B} matches.
If pattern {\tt B} matches 90 of the 100 strings in the set {\tt mA}, then {\tt B} is 90\% similar to {\tt A}.
If pattern {\tt A} only matches 50 of the strings in {\tt mB}, then {\tt A} is 50\% similar to {\tt B}.
We use similarity scores to create a similarity matrix as shown in Figure~\ref{fig:minimalMatrix}.
In row {\tt A}, column {\tt B} we see that {\tt B} is 90\% similar to {\tt A}.
In row {\tt B}, column {\tt A}, we see that {\tt A} is 50\% similar to {\tt B}.  Each pattern is always 100\% similar to itself, by definition.

Once the similarity matrix is built, the values of cells reflected across the diagonal of the matrix are averaged to create a half-matrix of undirected similarity edges, as illustrated in Figure~\ref{fig:matrixToGraph}.
This facilitates clustering using the  Markov Clustering (MCL) algorithm\footurl{http://micans.org/mcl/}.
We chose MCL  because it offers a fast and tunable way to cluster items by similarity and it is particularly useful when the number of clusters is not known \emph{a priori}.


In the implementation, strings are generated for each pattern using Rex~\cite{rex}.  Rex generates matching strings by representing the regular expression as an automaton, and then passing that automation to a constraint solver that generates members for it\footurl{http://research.microsoft.com/en-us/projects/rex/}.  If the regex matches a finite set of strings smaller than 400, Rex will produce a list of all possible strings.
Our goal is to generate 400 strings for each pattern to balance the runtime of the similarity analysis with the precision of the similarity calculations.

For clustering, we prune the similarity matrix to retain all similarity values greater than or equal to 0.75, setting the rest to zero, and then using MCL.
This threshold was selected based on recommendations in the MCL manual. The impact of lowering the threshold would likely result  in either the same number of more diverse clusters, or a larger number of clusters, but is unlikely to markedly change the largest clusters or their summaries, which are the focus of our analysis for \todoMid{some research question reference}.
, but further study is needed to substantiate this claim.
We also note that MCL can also be tuned using many parameters, including inflation and filtering out all but the top-k edges for each node.
After exploring the quality of the clusters using various tuning parameter combinations, the best clusters (by inspection) were found using an inflation value of 1.8 and k=83.   The top 100 clusters are categorized by inspection into six categories of behavior.

The end result is clusters and categories of highly behaviorally similar regular expressions, though we note that this approach has a tendency to over-approximate the similarity of two regexes. We measure similarity based on a finite set of generated strings, but some regexes  match an infinite set (e.g., \verb!ab*c!), so measuring similarity based on the first 400 strings may lead to an artificially high similarity value. To mitigate this threat, we chose a large number of generated strings for each regex, but future work includes exploring other approaches to computing regex similarity.


