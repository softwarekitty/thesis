
\section{Experiment design}

The overall idea of this study  is to present  programmers with one of several representations of semantically equivalent regexes and ask comprehension questions. By comparing the understandability of semantically equivalent regexes that have different representations, we aim to understand which representations  are more desirable and which are more smelly.
This study was implemented on Amazon's Mechanical Turk with 180 participants.  Each regex pattern was evaluated by 30 participants.
The patterns used were designed to belong to various representations in Figure~\ref{fig:refactoringTree}.

\begin{figure}[tb]
\centering
\includegraphics[width=0.8\columnwidth]{nontex/illustrations/exampleQuestion.eps}
\vspace{-12pt}
\caption{Example of one HIT Question}
\vspace{-6pt}
\label{fig:exampleQuestion}
\end{figure}

\input{table/matchingmetric}

\subsection{Metrics}
We measure the understandability of regexes using two complementary metrics, \emph{matching} and \emph{composition}.

\textbf{Matching:}
Given a pattern and a set of strings, a participant determines which strings will be matched by the pattern. There are four possible responses for each string, \emph{matches}, \emph{not a match}, \emph{unsure}, or blank. An example from our study is shown in Figure~\ref{fig:exampleQuestion}.

The percentage of correct responses, disregarding blanks and unsure responses, is the matching score.
For example, consider regex pattern \verb!`RR*'! and five strings shown in Table~\ref{matchingmetric}, and the responses from four participants in the \emph{P1}, \emph{P2}, \emph{P3} and \emph{P4} columns.
The oracle has the first three strings matching since they each contain at least one \verb!R! character. \emph{P1} answers correctly for the first three strings but incorrectly thinks the fourth string matches, so the matching score is $4/5 = 0.80$. \emph{P2} incorrectly thinks that the second string is not a match, so they also score $4/5 = 0.80$.  \emph{P3} marks `unsure' for the third string and so the total number of attempted matching questions is 4 instead of 5. \emph{P3} is incorrect about the second and fourth string, so they score $2/4 = 0.50$.  For \emph{P4}, we only have data for the first and second strings, since the other three are blank.  \emph{P4} marks `unsure' for the second matching question so only one matching question has been attempted, and it was answered correctly so the matching score is $1/1 = 1.00$.

Blanks were incorporated into the metric because questions were occasionally left blank in the study. Unsure responses were provided as an option so not to bias the  results when participants were honestly unsure of the answer. These situations did not occur very frequently. Only 1.1\% of the responses were left blank and only 3.8\% of the responses were marked as unsure.  We refer to a response with all blank or unsure responses as an `NA'. Out of 1800 questions, 1.8\%(32) were NA's (never more than 4 out of 30 per pattern).

\textbf{Composition:}
Given a pattern, a participant composes a string they think it matches. If the participant is accurate and the string indeed is matched by the pattern, then a composition score of 1 is assigned, otherwise 0.  For example, given the pattern \verb!`(q4fab|ab)'! from our study, the string, ``xyzq4fab" matches  and would get a score of 1, and the string, ``acb" does not match and would get a score of 0.

To determine a match, each pattern was compiled using the \emph{java.util.regex} library. A \emph{java.util.regex.Matcher} \verb!m! object was created for each composed string using the compiled pattern.  If \verb!m.find()! returned true, then that composed string was given a score of 1, otherwise it was given a score of 0.

\subsection{Implementation}
This study was implemented on the Amazon's Mechanical Turk (MTurk),  a crowdsourcing platform in which requesters can create human intelligence tasks (HITs) for completion by workers. Each HIT is designed to be completed in a fixed amount of time and workers are compensated with money if their work is satisfactory. Requesters can screen workers by requiring each to complete a qualification test prior to completing any HITs.

\subsubsection{Worker Qualification}
Workers qualified to participate in the study by answering questions regarding some basics of regex knowledge. These questions were multiple-choice and asked the worker to describe what the following patterns mean: \verb!`a+'!, \verb!`(r|z)'!, \verb!`\d'!, \verb!`q*'!, and \verb!`[p-s]'!. To pass the qualification, workers had to answer four of the five questions correctly.

\subsubsection{Selecting pairwise comparisons}
Using the patterns in the corpus as a guide, we created six metagroups containing three pairs of patterns focusing on:
\begin{itemize}
\item S1 vs S2
\item the digit default character class vs C1
\item the word default character class vs C1
\item negated digits and words vs C3, whitespace vs C2
\item additional vs kleene repetition
\item wrapping vs escaping literal characters
\end{itemize}
and four metagroups containing two triplets of patterns focusing on
\begin{itemize}
\item octal vs hex vs literal
\item D1 vs D2 vs D3
\item C1 vs C2 vs C5
\item octal vs literal and C2 vs C5
\end{itemize}

Each of these 10 metagroups contains 6 strings, resulting in a total of 60 regex patterns.  These patterns are logically partitioned into 26 semantic equivalence groups (18 from pairs, 8 from triples).

Although we had 42 pairwise comparisons (18 from pairs, 24 from triples),  we had to drop six comparisons  due to a design flaw since the patterns performed transformations from multiple equivalence classes. For example, pattern \verb!([\072\073])! is in C2 and T4, and was grouped with pattern \verb!(:|;)! in C5, T1, so it was not possible to attribute results purely to C2 and C5, or to T4 and T1. However, the third member of the group, \verb!([:;])!, could be compared with both, since it is a member of T1 and C2, so comparing it to \verb!([\072\073])! evaluates the transformation between T1 and T4, and comparing to \verb!(:|;)! evaluates the transformation between C2 and C5.

Another example of a pairwise comparison from a pair used in this study is a group with regexes \verb!`([0-9]+)\.([0-9]+)'! and  \verb!`(\d+)\.(\d+)'!, which is intended to evaluate the edge between C1 and C4.
An example of pairwise compairisons from a triple is a semantic group with regexes \verb!`((q4f){0,1}ab)'!, \verb!`((q4f)?ab)'!, and \verb!`(q4fab|ab)'! which is intended to explore the edges among D1, D2, and D3.

The end result is 36 pairwise comparisons across 14 edges from Figure~\ref{fig:refactoringTree}.

\subsubsection{Composing Tasks}
For each of the 26 groups of patterns, we created five strings, where at least one matched and at least one did not match. These strings were used to compute the matching metric.

Once all the patterns and matching strings were collected, we created tasks for the MTurk participants as follows:
randomly select a pattern from each of the 10 metagroups. Randomize the order of these 10 patterns, as well as the order of the matching strings for each pattern. After adding a question asking the participant to compose a string that each pattern matches, this creates one task on MTurk.   This process was completed until each of the 60 regexes appeared in 30 HITs, resulting in a total of 180 total unique HITs.
An example of a single regex pattern, the five matching strings and the space for composing a string is shown in Figure~\ref{fig:exampleQuestion}.
\subsubsection{Worker outcomes}
Workers were paid \$3.00 for successfully completing a HIT, and were only allowed to complete  one HIT.  The average completion time for accepted HITs was 682 seconds (11 mins, 22 secs).
A total of 55 HITs were rejected, and  of those, 48 were rushed through by one person leaving many answers blank, 4 other HITs were also rejected because a worker had submitted more than one HIT, one was rejected for not answering composition sections, and one was rejected because it was missing data for 3 questions.  Rejected HITs were returned to MTurk to be completed by others.
