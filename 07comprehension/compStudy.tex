
\section{Experiment design}

The overall idea of this study is to present programmers with one of several representations of semantically equivalent regexes and ask comprehension questions. By comparing the understandability of semantically equivalent regexes that have different representations, it should be possible to infer which representations are more desirable and which are more smelly.
This study was implemented on Amazon's Mechanical Turk with 180 participants.  Each regex was evaluated by 30 participants.
The regexes used were designed to belong to various nodes of the equivalence class graphs depicted in Figure~\ref{fig:refactoringTree}.

\begin{figure}[tb]
\centering
\includegraphics[width=0.8\columnwidth]{nontex/illustrations/exampleQuestion.eps}
\vspace{-12pt}
\caption{Example of one HIT Question}
\vspace{-6pt}
\label{fig:exampleQuestion}
\end{figure}

\input{table/matchingmetric}

\subsection{Metrics}
The understandability of regexes was measured using two complementary metrics, \emph{matching} and \emph{composition}.

\textbf{Matching:}
Given a regex and a set of strings, a participant determines which strings will be matched by the regex. There are four possible responses for each string, \emph{matches}, \emph{not a match}, \emph{unsure}, or blank. An example from our study is shown in Figure~\ref{fig:exampleQuestion}.  The use of the term `matches' in this chapter is consistent with the meaning described in Section~\ref{sec:matchingDefined} - if any substring of a target string belongs to the set of strings specified by a particular regex, then that regex is said to \emph{match} that target string.  For ease of expression, a string is said to \emph{match} a regex if that regex matches the string.

The percentage of correct responses, disregarding blanks and unsure responses, is the matching score.
For example, consider regex \cverb!RR*! and five strings shown in Table~\ref{matchingmetric}, and the responses from four participants in the \emph{P1}, \emph{P2}, \emph{P3} and \emph{P4} columns.
The oracle has the first three strings matching since they each contain at least one \verb!`R'! character. \emph{P1} answers correctly for the first three strings but incorrectly thinks the fourth string matches, so the matching score is $4/5 = 0.80$. \emph{P2} incorrectly thinks that the second string is not a match, so they also score $4/5 = 0.80$.  \emph{P3} marks `unsure' for the third string and so the total number of attempted matching questions is 4 instead of 5. \emph{P3} is incorrect about the second and fourth string, so they score $2/4 = 0.50$.  For \emph{P4}, we only have data for the first and second strings, since the other three are blank.  \emph{P4} marks `unsure' for the second matching question so only one matching question has been attempted, and it was answered correctly so the matching score is $1/1 = 1.00$.

Blanks were incorporated into the metric because questions were occasionally left blank in the study. Unsure responses were provided as an option so not to bias the  results when participants were honestly unsure of the answer. These situations did not occur very frequently. Only 1.1\% of the responses were left blank and only 3.8\% of the responses were marked as unsure.  Response with all blank or unsure responses are referred to as an `NA'. Out of 1800 questions, 1.8\%(32) were NA's (never more than 4 out of 30 per regex).

\textbf{Composition:}
Given a regex, a participant composes a string they think it matches. If the participant is accurate and the string indeed is matched by the regex, then a composition score of 1 is assigned, otherwise 0.  For example, given the regex \cverb!(q4fab|ab)! from our study, the string \verb!"xyzq4fab"! matches and would get a score of 1, and the string \verb!"fac"! is not matched and would get a score of 0.

To determine a match, each regex was compiled using the \emph{java.util.regex} library. A \emph{java.util.regex.Matcher} {\tt m} object was created for each composed string using the compiled regex.  If {\tt m.find()} returned true, then that composed string was given a score of 1, otherwise it was given a score of 0.

\subsection{Implementation}
This study was implemented on Amazon's Mechanical Turk (MTurk),  a crowdsourcing platform in which requesters can create human intelligence tasks (HITs) for completion by workers. Each HIT is designed to be completed in a fixed amount of time and workers are compensated with money if their work is satisfactory. Requesters can screen workers by requiring each to complete a qualification test prior to completing any HITs.

\subsubsection{Worker Qualification}
Workers qualified to participate in the study by answering questions regarding some basics of regex knowledge. These questions were multiple-choice and asked the worker to describe what the following regexes mean: \cverb!a+!, \cverb!(r|z)!, \cverb!\d!, \cverb!q*!, and \cverb![p-s]!. To pass the qualification, workers had to answer four of the five questions correctly.

\subsubsection{Selecting pairwise comparisons}

 Using the regexes in the corpus as a guide, ten metagroups were created for this study.  The first six metagroups (re-numbered for simplicity) each contain three pairs of regexes, as shown in the following six lists:

Metagroup 1: testing S1 vs S2
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[S1] \cverb!%([0-9A-Fa-f]{2})!
\item[S2] \cverb!%([0-9a-fA-F][0-9a-fA-F])!
\item[S1] \cverb!&d([aeiou]{2})z!
\item[S2] \cverb!&d([aeiou][aeiou])z!
\item[S1] \cverb!fa[lmnop]{3}!
\item[S2] \cverb!fa[lmnop][lmnop][lmnop]!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 2: testing C1 vs C4, focusing on DEC
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C1] \cverb!([0-9]+)\.([0-9]+)!
\item[C4] \cverb!(\d+)\.(\d+)!
\item[C1] \cverb!xg1([0-9]{1,3})%!
\item[C4] \cverb!xg1(\d{1,3})%!
\item[C1] \cverb![a-f]([0-9]+)[a-f]!
\item[C4] \cverb![a-f](\d+)[a-f]!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 3: testing C1 vs C4, focusing on WRD
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C1] \cverb!&([A-Za-z0-9_]+);!
\item[C4] \cverb![&(\w+);]!
\item[C1] \cverb!1q[A-Za-z0-9_][A-Za-z0-9_]!
\item[C4] \cverb![1q\w\w]!
\item[C1] \cverb![tuv[A-Za-z0-9_]]!
\item[C4] \cverb![tuv\w]!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 4: C4 vs (C3 or C2), covering the other defaults
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C3] \cverb![^0-9A-Za-z]!
\item[C4] \cverb![\W_]!
\item[C3] \cverb![^0-9]!
\item[C4] \cverb![\D]!
\item[C2] \cverb![\t\r\f\n ]!
\item[C4] \cverb![\s]!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 5: testing L2 vs L3 (note that the pair on the left is not equivalent, due to an oversight - the first regex was meant to be \cverb!\.\.*!)
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[L2] \cverb!\..*!
\item[L3] \cverb!\.+!
\item[L2] \cverb!zaa*!
\item[L3] \cverb!za+!
\item[L2] \cverb!RR*!
\item[L3] \cverb!R+!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 6: testing T1 vs T3
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[T1] \cverb!(\$\{)\d+(:[^}]+\})!
\item[T3] \cverb!([$][{])\d+(:[^}]+[}])!
\item[T1] \cverb!t\.\$+\d+\*!
\item[T3] \cverb!t[.][$]+\d+[*]!
\item[T1] \cverb!\{\$(\d+\.\d)\}!
\item[T3] \cverb![{][$](\d+[.]\d)[}]!
\end{small}
\end{itemize}
\end{multicols}

Four additional metagroups were created, each containing two sets of three equivalent regexes.  These groups of three were useful because more comparisons could be drawn from the same number of experiments.  The regexes used are shown in the following four lists:

Metagroup 7: testing D1 vs D2 vs D3
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[D1] \cverb!((q4f){0,1}ab)!
\item[D1] \cverb!(dee(do){1,2})!
\item[D2] \cverb!((q4f)?ab)!
\item[D2] \cverb!(deedo(do)?)!
\item[D3] \cverb!(q4fab|ab)!
\item[D3] \cverb!(deedo|deedodo)!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 8: testing C1 vs C2 vs C5
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C1] \cverb!no[w-z]5!
\item[C1] \cverb!tri[a-f]3!
\item[C2] \cverb!no[wxyz]5!
\item[C2] \cverb!tri[abcdef]3!
\item[C5] \cverb!no(w|x|y|z)5!
\item[C5] \cverb!tri(a|b|c|d|e|f)3!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 9: testing C2/T1 vs C5/T1 vs C2/T4 (provides T1 vs T4 and C2 vs C5)
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C2/T1] \cverb!([}{])!
\item[C2/T1] \cverb!([:;])!
\item[C5/T1] \cverb!(\{|\})!
\item[C5/T1] \cverb!(:|;)!
\item[C2/T4] \cverb!([\072\073])!
\item[C2/T4] \cverb!([\0175\0173])!
\end{small}
\end{itemize}
\end{multicols}
\vspace{-2mm}
Metagroup 10: testing C1/T2 vs C1/T4 vs C2/T1 (provides only T2 vs T4)
\vspace{-5mm}
\begin{multicols}{3}
\begin{itemize}[noitemsep,topsep=0pt]
\begin{small}
\item[C1/T2] \cverb!xyz[\x5b-\x5f]!
\item[C1/T2] \cverb!t[\x3a-\x3b]+p!
\item[C1/T4] \cverb!xyz[\0133-\0140]!
\item[C1/T4] \cverb!t[\072-\073]+p!
\item[C2/T1] \cverb!xyz[_\[\]`\^\\]!
\item[C2/T1] \cverb!t[:;]+p!
\end{small}
\end{itemize}
\end{multicols}

Each of these 10 metagroups contains 6 regexes, resulting in a total of 60 regexes.  These regexes are logically partitioned into 26 semantic equivalence groups (18 from pairs, 8 from triples).

Although this design provides 42 pairwise comparisons (18 from pairs, 24 from triples),  six comparisons had to be dropped due to a design flaw since the regexes performed transformations from multiple equivalence classes. For example \cverb!([\072\073])! is in C2 and T4.  This regex was paired with \cverb!(:|;)! in C5, T1, so it was not possible to attribute results purely to C2 and C5, or to T4 and T1. However, the third member of the group, \cverb!([:;])!, could be compared with both, since it is a member of T1 and C2, so comparing it to \cverb!([\072\073])! evaluates the transformation between T1 and T4, and comparing to \cverb!(:|;)! evaluates the transformation between C2 and C5.  Also, the first pair of regexes from metagroup 5: \cverb!\..*! and \cverb!\.+! are not equivalent.  The first regex was meant to be \cverb!\.\.*!.  Data gathered for this pairing was ignored.

Another example of a pairwise comparison from a pair used in this study is a group with regexes \cverb!([0-9]+)\.([0-9]+)! and  \cverb!(\d+)\.(\d+)!, which is intended to evaluate the edge between C1 and C4.
An example of pairwise comparisons from a triple is a semantic group with regexes \cverb!((q4f){0,1}ab)!, \cverb!((q4f)?ab)!, and \cverb!(q4fab|ab)! which is intended to explore the edges among D1, D2, and D3.

The end result is 35 pairwise comparisons across 14 edges from Figure~\ref{fig:refactoringTree}.

\subsubsection{Composing Tasks}
For each of the 26 groups of regexes, five strings were created, where at least one matched and at least one did not match. These strings were used to compute the matching metric.

Once all the regexes and matching strings were collected, we created tasks for the MTurk participants as follows:
randomly select a regex from each of the 10 metagroups. Randomize the order of these 10 regexes, as well as the order of the matching strings for each regex. After adding a question asking the participant to compose a string that each regex matches, this creates one task on MTurk.   This process was completed until each of the 60 regexes appeared in 30 HITs, resulting in a total of 180 total unique HITs.
An example of a single regex, the five matching strings and the space for composing a string is shown in Figure~\ref{fig:exampleQuestion}.
\subsubsection{Worker outcomes}
Workers were paid \$3.00 for successfully completing a HIT, and were only allowed to complete  one HIT.  The average completion time for accepted HITs was 682 seconds (11 mins, 22 secs).
A total of 55 HITs were rejected, and  of those, 48 were rushed through by one person leaving many answers blank, 4 other HITs were also rejected because a worker had submitted more than one HIT, one was rejected for not answering composition sections, and one was rejected because it was missing data for 3 questions.  Rejected HITs were returned to MTurk to be completed by others.
