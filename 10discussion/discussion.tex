\chapter{DISCUSSION}
% \label{sec:finalDiscussion}

Our contributions are:
\begin{itemize}
\item Identification of  equivalence classes for regular expressions with possible transformations within each class,
\item Conducted an empirical study with 180 participants evaluating regex understandability,
\item Conducted an empirical study identifying opportunities for regex refactoring  in Python projects based on how regexes are expressed, and
\item {Identified preferred regex representations and refactorings that are the most understandable and conform best to community standards, backed by empirical evidence.}
\end{itemize}

\section{Implications of the thesis as a whole}
\subsection{Review of implications already discussed}
\subsection{Implications considering all experiments together}

\section{Opportunities for future work studying regular expressions}
\subsection{Semantic search}
\subsection{Ephemeral regex}
\subsection{Comparing regex usage across communities}
\subsection{Evolution of patterns}
\subsection{Taxonomy of regex language varieties}

\subsubsection{Capturing Specific Content Near A Delimiter}
The survey results from Section
\todoLast{N} indicate that capturing parts of strings is among the most frequent activities for which developers use regexes.
From a feature perspective, the capture group (CG) is the most frequently used in terms of patterns (Table~\ref{table:featureStats}).  This feature has two functions: 1) logical grouping as would be expected by parenthesis, and 2) retrieval of information in one logical grouping.  As mentioned in Section
\todoLast{N}, capturing content was a primary goal evident in several cluster categories.  The fourth-largest category is based entirely on capturing the content between brackets or parentheses (Section\todoLast{N}).

Many uses of CG also use the ANY and KLE features, eg. \verb!(.*){(.*)}(.*)! and \verb!\\s*([^: ]*)\\s*:(.*)!.  This type of usage frequently revolves around an important delimiter character such as \verb!:! or \verb!\!.  This use case is well supported by existing tools for ASCII characters, but future tools should consider the centrality of this use case and its implications for non-English users of regex tools.  For example, Unicode characters like `U+060D' the Arabic Date Separator, or `U+1806' the Mongolian Todo Soft Hyphen may be used to locate segments of text that a user would want to capture.


\subsubsection{Counting Lines}
Text files containing one unit of information per line are common in a wide variety of applications (for example .log and .csv files).  Out of the 13,597 patterns in the corpus, 3,410 (25\%) contained ANY followed by KLE  (i.e., \verb!`.*'!), often at the end of the pattern.
One reasonable explanation for this tendency to put \verb!`.*'! at the end of a pattern is that users want to disregard all matches after the first match on a single line in order to count how many distinct lines the match occurs on.  Survey participants indicated an average frequency of ``Counting lines that match a pattern" and ``Counting substrings that match a pattern" at 3.2 or rarely/occasionally. It may be valuable for tool builders to include support for common activities such as line counting.


\subsubsection{Refactoring Regexes}
The survey showed that users want readability and find the lack of readable regexes to be a major pain point.
This provides an opportunity to introduce refactoring transformations to enhance readability or comprehension.
As one opportunity, certain character classes are logically equivalent and can be expressed differently, for example, \verb!\d! $\equiv$ \verb![0123456789]! $\equiv$ \verb![0-9]!. While \verb!\d! is more succinct, \verb![0-9]! may be easier to read, so a refactoring for \emph{default to custom character classes} could be introduced.
Human studies are needed to evaluate the readability and comprehension of various regex features in order to define and support appropriate regex refactorings.

Another avenue of refactoring could be for performance. Various implementations of regex libraries may perform more efficiently with some features than others. An evaluation of regex feature implementation speeds would facilitate semantic transformations based on performance, similar to performance refactorings for LabVIEW~\cite{chambers2013smell, chambers2015impact}.

Additionally, some developers may  \emph{find} specific content with a regex  and then subsequently \emph{capture} it with string parsing, which may be more error prone than using a capture group and indicates a missed opportunity to use the full extent of regex libraries. Future work will explore source code to identify the frequency of such occurrences and design refactorings to better utilize regex library features.

\vspace{-2pt}
\subsubsection{Migration  Support for Developers}
Within standard programming languages, regular expressions libraries are very common, yet there are subtle  differences between language libraries in the supported features. For example, Java supports possessive quantifiers like \verb! `ab*+c'! (here the `+' is modifying the `*' to make it possessive) whereas Python does not. Differences among programming language implementations was identified as a pain point for using regular expressions by 17\% of the survey participants. This provides a future opportunity for tools that translate between regex utilizations in various languages.

\vspace{-2pt}
\subsubsection{Similarity Beyond String Matching}
There are various ways to compute similarity between regexes, each with different tradeoffs.
While the similarity analysis we employ over-approximates similarity when compared to containment analysis, it may under-approximate similarity in another sense.

For example, two regexes that have dissimilar matching behavior could be very similar in purpose and in the eyes of the developer. For example, \verb!commit:\[(\d+)\] - (.*)! and \verb!push:\[(\d+)\] - (.*)! could both be used to  capture the id and command from a versioning system, but match very different sets of strings. Future work would apply abstractions to the regex strings, such as removing or relaxing literals, prior to similarity analysis to capture and cluster such similarities.

From another perspective, our regex similarity measure, and even containment analysis, could treat behaviorally identical regexes as the same, when  their usage in practice is completely different. For example, in Table~\ref{table:exampleCluster}, the regexes \verb!`:+'! and \verb!`(:+)'! are behaviorally identical in that they match the same strings, except the latter uses a capture group. In practice, these may be used very differently, where the former may be used for validation and the latter for extraction. This usage difference could be observed by code  analysis, and is left for future work.

\vspace{-2pt}
\subsubsection{Automated Regex Repair}
Regular expression errors are common and have produced thousands of bug reports~\cite{Spishak:2012:TSR:2318202.2318207}. This provides an opportunity to introduce automated repair techniques for regular expressions.
Recent approaches to automated program repair rely on mutation operators to make small changes to source code and then re-run the test suite (e.g., ~\cite{cacm10, genprog-tse-journal}). In regular expressions, it is likely that the broken regex is close, semantically, to the desired regex. Syntax changes through mutation operators could lead to big changes in behavior, so we hypothesize that using the semantic clusters identified in Section\todoLast{N} to identify potential repair candidates could efficiently and effectively converge on a repair candidate.

\subsubsection{Developer Awareness of Best Practices}
One category of clusters, \emph{Content of Brackets and Parenthesis}, parses the contents of angle brackets, which may indicate developers are using regexes to parse HTML or XML.  As the contents of angle brackets are usually unconstrained, regexes are a poor replacement for XML or HTML parsers.  This may be a missed opportunity for the regex users to take advantage of more robust tools. More research is needed into how regex users discover best practices and how aware they are of how regexes should and should not be used.

\subsubsection{Tool-Specific Regex Exploration}
In some environments, such as command line or text editor, regexes are used extensively by the surveyed developers (Section\todoLast{N}), but these regular expressions do not persist. Thus, using a repository analysis for feature usage only illustrates part of how regexes are used in practice. Exploring how the feature usage differs between environments would help inform tool developers about how to best support regex usage in context, and is left for future work.

Based on our analyses of source code and our empirical study on the understandability of regex representations, we have identified preferred regex representations that may make regexes easier to understand and thus maintain. In this section, we describe the implications of these results.

\subsection{Interpreting Results}
In the CCC equivalence class, C1 (e.g., \verb![0-9a]!) is more commonly found in the patterns and projects.  Representations C2 (e.g., \verb![0123456789a]!) and C3 (e.g., \verb![^\x00-/:-`b-\x7F]!) appear in similar percentages of patterns and projects but there is no significant difference in understandability considering two pairs of regexes tested as part of E13 (Table~\ref{table:testedEdgesTable}). However, a small preference is shown for C1 over C2 (E7), leading this to to be the winner of both the community support and understandability analyses.    Regex length is probably important for understandability, though we did not test for this.

\todoNow{the longest regex in the corpus is X characters long...}

In the DBB group, D3 (e.g., \verb!pBs|pBBs|pBBBs!) merits further exploration because it is the most understandable but least common node in DBB group.  This may be because explicitly listing the possibilities with an OR is easy to grasp, but if the number of items in the OR is too large, the understandability may go down. Further analysis is needed to determine the optimal thresholds for representing a regex as D3 compared to D1 (e.g., \verb!pB{1,3}s!) or D2 (e.g., \verb!pBB?B?s!).

\todoLast{Intuitively, it seems that D2 may be more common because 0,1 is just a more common use case than an arbitrary range like 4, 25.}

In the SNG group, S1 is a compact representation (e.g., \verb!S{3}!), but S2 was preferred (e.g., \verb!SSS!). Similar to the DBB group, this may be do to the particular examples chosen in the analysis, as a large number of explicit repetitions may not be as preferred.

In the LWB group,  L1 (e.g., \verb!A{2,}!) is rare, appearing in $<1$\% of the patterns. Representations L2 (e.g., \verb!AAA*!) and L3 (e.g., \verb!AA+!) appear in similar numbers of patterns and projects, but there is a significant difference in their understandability, favoring L3.
\todoLast{it's clear that this is a rare use case, and also that L3 is the most common  use case.  Patterns using star are secondary, helper patterns because they will trivially match anything, so they are less common.  But anyway...}

\todoLast{S2 is over-weighted because of double-characters in regular words like foot.}
In the LIT group, T1 (e.g., \verb!\a\$>!) is the typical way to list literals, but the reason to use hex (T2) or oct (T4) types is because some characters cannot be represented any other way, like invisible chars.  One main result of our work is that  T4 (e.g., \verb!\007\036\062!) is  less understandable   than T2 (e.g., \verb!\x07\x24\x3E!), so if invisible chars are required, hex is the more understandable representation.
Regarding T3 (e.g., \verb!\a[$]>!), initially we thought the square brackets would be more understandable than using an escape character,  but we found the opposite. Given a choice between T1 and T3, the escape character was more understandable.

\subsection{Opportunities For Future Work}
There are several directions for future work related to regex study and refactoring.

\paragraph{Regex Refactoring for Performance}
The representation of regexes may have a strong impact on the runtime performance of a chosen regex engine. Prior work has sought to expedite the processing of regexes over large bodies of text~\cite{Baeza-Yates:1996:FTS:235809.235810}.
Refactoring regexes for performance would complement those efforts.
Further study is needed to determine which representations are most efficient, leading to a whole new area of study on regex refactoring for performance, a topic already explored for
Depending on the efficiency of an organization's chosen regex engine, an organization may want to enforce standards for efficiency.
, or for compatibility with a regex analysis tool like Z3, HAMPI, BRICS or REX.

\paragraph{Regex Migration Libraries}
We have identified opportunities
 to improve the understandability of regexes in existing code bases by looking for some of the less understandable regex representations, which can be thought of as antipatterns, and refactoring to the more common or understandable representations.
 Building migration libraries is a promising direction of future work to ease the manual burden of this process, similar in spirit to prior work on class library migration~\cite{Balaban:2005:RSC:1103845.1094832}.

\paragraph{Regex Refactoring Applications}
Maintainers of code that is intentionally obfuscated for security purposes may want to develop regexes that they understand and then automatically transform them into the least understandable regex possible.

One fundamental concept that many users of regex struggle to learn is when to use regexes for simple parsing, and when to write a full-fledged parser (for example, when parsing HTML).  Regexes that are trying to parse HTML, XML or similar languages could be refactored not into a better regex, but into some code with an equivalent intention that does parsing much better.


% \paragraph{Equivalence Class Models}
% We looked at five equivalence classes, each with three to five nodes.
% Future work could consider richer models with more or different classes and nodes.
% \todoLast{For example, we have looked at all ranges as equivalent, all defaults as equivalent, and relied on many such generalizations.  However, the range }
% \verb![a-f]!
% \todoLast{is likely to be more understandable for most people than a range like}
%  \verb![:-`]!.

% Additional equivalence groups to consider may include:
% \begin{description} \itemsep -2pt
% \item[Single line option]  \verb!'''(.|\n)+'''! $\equiv$ \verb!(?s)'''(.)+'''!
% \item[Multi line option]  \verb!(?m)G\n! $\equiv$ \verb!(?m)G$!
% \item[Case insensitive]  \verb!(?i)[a-z]! $\equiv$ \verb![A-Za-z]!
% \item[Backreferences]  \verb!(X)q\1! $\equiv$ \verb!(?P<name>X)q\g<name>!
% \item[Word Boundaries]  \verb!\bZ! $\equiv$ \verb!((?<=\w)(?=\W)|(?<=\W)(?=\w))Z!
% \end{description}

% It might also be the case that there exist critical comprehension differences within a representation. For example, between C1 (e.g., \verb![0-9a]!) and C4 (e.g., \verb![\da]!), it could be the case that \verb![0-9]! is preferred to \verb![\d]!, but \verb![A-Za-z0-9_]! is not be preferred to \verb![\w]!).
% By creating a more granular model of equivalence classes, and making sure to carefully evaluate alternative representations of the most frequently used specific patterns,  additional useful refactorings could be identified.

% \todoLast{
% One of the most straightforward ways to address understandability is to directly ask software professionals which from a list of equivalent regexes they prefer and why.
% If understandability measurements used regexes sampled from the codebase of a specific community(most frequently observed regexes, most buggy regexes, regexes on the hottest execution paths, etc.), and measured the understanding of programming professionals working in that community, then the measurements and the refactorings they imply would be more likely to have a direct and certain positive impact.
% In another study, we did a survey where software professionals indicated that understandability of regexes they find in source code is a major pain point.  In this study, our participants indicated that they read about twice as many regexes as they compose.  What is the impact on maintainers, developers and contributors to open-source projects of not being able to understand a regex that they find in the code they are working with?  Presumably this is a frustrating experience - how much does a confusing regex slow down a software professional?  What bugs or other negative factors can be attributed to or associated with regexes that are difficult to understand?  How often does this happen and in what settings?  Future work could tailor an in-depth exploration of the overall costs of confusing regexes and the potential benefits of refactoring or other treatments for confusing regexes.}

% \paragraph{Regex Migration Libraries}
% We have identified opportunities
%  to improve the understandability of regexes in existing code bases by looking for some of the less understandable regex representations, which can be thought of as antipatterns, and refactoring to the more common or understandable representations.
%  Building migration libraries is a promising direction of future work to ease the manual burden of this process, similar in spirit to prior work on class library migration~\cite{Balaban:2005:RSC:1103845.1094832}.

% \paragraph{Regex Refactoring Applications}
% Maintainers of code that is intentionally obfuscated for security purposes may want to develop regexes that they understand and then automatically transform them into the least understandable regex possible.

% One fundamental concept that many users of regex struggle to learn is when to use regexes for simple parsing, and when to write a full-fledged parser (for example, when parsing HTML).  Regexes that are trying to parse HTML, XML or similar languages could be refactored not into a better regex, but into some code with an equivalent intention that does parsing much better.

% \paragraph{Regex Programming Standards}
% Many organizations enforce coding standards in their repositories to ease understandability.
% Presently, we are not aware of coding standards for regular expressions, but this work suggests that enforcing standard representations for various regex constructs could ease comprehension.

% \paragraph{Regex Refactoring for Performance}
% The representation of regexes may have a strong impact on the runtime performance of a chosen regex engine. Prior work has sought to expedite the processing of regexes over large bodies of text~\cite{Baeza-Yates:1996:FTS:235809.235810}.
% Refactoring regexes for performance would complement those efforts.
% Further study is needed to determine which representations are most efficient, leading to a whole new area of study on regex refactoring for performance, a topic already explored for
% Depending on the efficiency of an organization's chosen regex engine, an organization may want to enforce standards for efficiency.
% , or for compatibility with a regex analysis tool like Z3, HAMPI, BRICS or REX.

% \subsection{Threats to Validity}

% \textbf{Internal}
% We measure understandability of regexes using two metrics, matching and composition. However, these measures may not reflect actual understanding of the regex behavior. For this reason, we chose to use two metrics and present the analysis in the context of reading and writing regexes, but the threat remains.

% Participants evaluated regular expressions during tasks on MTurk, which may not be representative enough of the context in which programmers would encounter regexes in practive. Further study is needed to determine the impact of the experimentation context on the results.

% Some regex representations from the equivalence classes were not involved in the understandability analysis and that may have biased the results against those nodes. Repetition of the analysis with more compete coverage of the edges in the equivalence classes is needed.

% We treated unsure responses as omissions that  did not count  against the matching scores. Thus, if a participant answered two strings correctly and marked the other three strings as unsure, then this was 2/2 correct, not 2/5. This may have inflated the matching scores, however, less than 5\% of the matching scores were impacted by such responses.

% \todoLast{In our analyses, we measure understandability using matching and composition metrics.
% However, there may be other ways to approach regex understandability, such as deciding which regexes in a set are equivalent, finding the minimum modification to some text so that a given regex will match it.
% It may also be meaningful to provide some code that exists around a regex as context, since that would better represent a scenario in which programmers would encounter regexes in practice.
% Further study is needed to determine if the chosen metrics and experimentation context have resulted in a reasonable measure of understandability.}

% \textbf{External}
% Participants in our survey came from MTurk, which may not be representative of people who read and write regexes on a regular basis.

% The regexes  used in the evaluation were inspired by those found in Python code, which is just one language that has library support for regexes. Thus, we may have missed opportunities for other refactorings based on how programmers use regexes in other programming languages.

% The results of the understandability analysis may be closely tied to the particular regexes chosen for the experiment. For many of the representations, we had several comparisons. Still, replication with more regex patterns is needed.% to validate our results.

% \todoMid{what about the threat of too few examples per node?  Didn't cover every edge.  Regex set is randomly collected online, not focused on any specific target audience.}

% \todoLast{Our community analysis only focuses on the Python language, but as the vast majority of regex features are shared across most general programming languages (e.g., Java, C, C\#, or Ruby), a Python {pattern} will (almost always) behave the same when used in other languages and our results are likely to generalize.
% , whereas a utilization is not universal in the same way (i.e., it may not compile in other languages, even with small modifications to function and flag names).
% As an example, the {\tt re.MULTILINE} flag, or similar, is present in Python, Java, and C\#, but  the Python {\tt re.DOTALL} flag is not present in C\# though it has an equivalent flag in Java.}

% \todoMid{Looks like M0, M1, M2, M3 and M9 are very dependent on the regex chosen, so regex-specific refactorings like:}

% 0.1401 \verb!&d([aeiou][aeiou])z'    &d([aeiou]{2})z'!
% 0.075   \verb![\t\r\f\n ]'    [\s]'!
% 0.1024  \verb![a-f]([0-9]+)[a-f]' [a-f](\d+)[a-f]'!
% 0.1271  \verb![\{][\$](\d+[.]\d)[}]'!
% \verb!\\\{\\\$(\d+\.\d)\}'!
% \todoMid{(from M0,M1,M2,M9 respectively) have okay P-values and may indicate regex-specific refactorings, but do not indicate an overall trend for that type of refactoring.
% Notice that M3 does not even have a strong p-value candidate, but this may be thrown off because of the very confusing regex chosen for CCC:
% 0.78    0.79}
% \verb!xyz[_\[\]`\^\\]'!    \verb!xyz[\x5b-\x5f]'!
% \todoMid{which has a lot of escape characters, so that the hex group was easier to understand than the CCC.}

% \todoMid{Meanwhile M4,M5 and M7 have both ambiguous p-values and anova results.  But this is still a finding: that no refactoring is needed between things like:}
% \verb!(q4fab|ab)'! (\verb!(q4f){0,1}ab)'!
% \verb!tri[abcdef]3'!   \verb!tri(a|b|c|d|e|f)3'!
% \verb!&(\w+);'!    \verb!&([A-Za-z0-9_]+);'!
% \todoMid{(from M4,M5,M7 respectively)  Although one refactoring from M5 might be of slight interest:}
% 0.1196  FALSE   \verb!tri[a-f]3'!  \verb!tri(a|b|c|d|e|f)3'!


\subsection{Reason for knowledge gap}
\todoLast{clean and move this}
What explains the lack of rigor for regular expressions?  I've been pondering this for quite a while now.  This seems like it might be a case of being unable to see the forest because of the trees.  Everyone in CS uses regex, and deals to some extent with other people using regex.  It's never billed as top priority to optimize this, and it's so fundamental that the wikipedia pages explaining regex and Klene's theory are nearly circular with definitions of regular expressions relying on knowledge of what regular expressions are.  The terminology is also very confusing to theory people, who probably believe that modern regex still represent DFA's and regular languages.  I'd say there is an over-abundance of anecdotal evidence about how regex are used, so that people believe that it is a known topic.  I also suspect a very real hacker/Unix ethos, where the odd ducks out there who really love regex also scorn formal evaluations and `design by committee' in favor of just getting things to work, getting it good enough.  It has flabbergasted me to look at every single documentation source for the dozens of different regex language `flavors' and see that it is always just a thrown-together hodgepodge of examples and concepts.  Most regex doc feels like it may be incomplete, saying stuff like `it's mostly like PCRE'.  Nobody and I mean nobody has a definitive feature list, outlining how the flavors differ and overlap.  Like it's just okay to have all these dangling details for something that is pretty confusing already.  Now you have two problems.  It would be pretty fun to do a sort of gotcha-quiz of developers to see how many people would fall for the common myths and stuff.



